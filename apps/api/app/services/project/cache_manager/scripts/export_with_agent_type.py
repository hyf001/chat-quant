#!/usr/bin/env python3
"""
æ™ºèƒ½ä½“ç±»å‹çº§åˆ«å¯¼å‡ºè„šæœ¬

æ ¹æ®æ™ºèƒ½ä½“ç±»å‹è‡ªåŠ¨å¯¼å‡ºè¯¥ç±»å‹ä¸‹æ‰€æœ‰ç›¸å…³çš„æ•°æ®èµ„æºã€‚

ç”¨æ³•:
    apps/api/.venv/bin/python export_with_agent_type.py [--agent-type TYPE] [--business-lines BL1,BL2] [--dry-run] [--verbose]

ç¤ºä¾‹:
    # å¯¼å‡ºdata_analysisæ™ºèƒ½ä½“ç±»å‹çš„æ‰€æœ‰æ•°æ®ï¼ˆæ•°æ®è¡¨ + BIæŠ¥è¡¨ï¼‰
    apps/api/.venv/bin/python export_with_agent_type.py --agent-type data_analysis

    # å¯¼å‡ºdata_developæ™ºèƒ½ä½“ç±»å‹çš„æ‰€æœ‰æ•°æ®ï¼ˆæ•°æ®æº + ä¼ è¾“ä»»åŠ¡ï¼‰
    apps/api/.venv/bin/python export_with_agent_type.py --agent-type data_develop

    # ä»…å¯¼å‡ºiwcä¸šåŠ¡çº¿çš„æ‰€æœ‰æ•°æ®
    apps/api/.venv/bin/python export_with_agent_type.py --agent-type data_analysis --business-lines iwc

    # é¢„è§ˆæ¨¡å¼ï¼ˆä¸å®é™…å†™å…¥æ–‡ä»¶ï¼‰
    apps/api/.venv/bin/python export_with_agent_type.py --agent-type data_analysis --dry-run

    # è¯¦ç»†è¾“å‡º
    apps/api/.venv/bin/python export_with_agent_type.py --agent-type data_analysis --verbose
"""
import sys
import asyncio
import logging
import argparse
from pathlib import Path
from typing import List, Optional

# åŠ¨æ€æŸ¥æ‰¾å¹¶æ·»åŠ apps/apiç›®å½•åˆ°sys.path
def find_api_root():
    """å‘ä¸ŠæŸ¥æ‰¾apps/apiç›®å½•"""
    current = Path(__file__).resolve().parent
    while current.parent != current:  # é˜²æ­¢åˆ°è¾¾æ ¹ç›®å½•
        # æ£€æŸ¥æ˜¯å¦æ˜¯apiç›®å½•ï¼ˆåŒ…å«appå­ç›®å½•ï¼‰
        if (current / "app").exists() and (current / "app").is_dir():
            # éªŒè¯è¿™æ˜¯æ­£ç¡®çš„apiç›®å½•ï¼ˆåŒ…å«cache_managerï¼‰
            if (current / "app" / "services" / "project" / "cache_manager").exists():
                return current
        current = current.parent
    raise RuntimeError("æ— æ³•æ‰¾åˆ°apps/apiç›®å½•ï¼Œè¯·ç¡®ä¿è„šæœ¬åœ¨æ­£ç¡®çš„é¡¹ç›®ç»“æ„ä¸­è¿è¡Œ")

sys.path.insert(0, str(find_api_root()))

from app.services.project.cache_manager.config.config_manager import ConfigManager
from app.services.project.cache_manager.config.constants import get_agent_type_config
from app.services.project.cache_manager.exporters.data_table_exporter import DataTableExporter
from app.services.project.cache_manager.exporters.report_exporter import ReportExporter
from app.services.project.cache_manager.exporters.datasource_exporter import DataSourceExporter
from app.services.project.cache_manager.exporters.transmission_task_exporter import TransmissionTaskExporter


def setup_logging(verbose: bool = False):
    """è®¾ç½®æ—¥å¿—"""
    level = logging.DEBUG if verbose else logging.INFO
    logging.basicConfig(
        level=level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )


async def export_data_analysis(
    config: ConfigManager,
    agent_type: str,
    business_lines: Optional[List[str]] = None,
    dry_run: bool = False,
    verbose: bool = False,
    query: Optional[str] = None,
    batch_size: int = 10
) -> bool:
    """
    å¯¼å‡ºæ•°æ®åˆ†ææ™ºèƒ½ä½“ç±»å‹çš„æ‰€æœ‰æ•°æ®

    åŒ…æ‹¬ï¼š
    - æ•°æ®è¡¨ï¼ˆPHYSICALï¼‰
    - BIæŠ¥è¡¨ï¼ˆREPORTï¼‰

    Args:
        config: é…ç½®ç®¡ç†å™¨
        agent_type: æ™ºèƒ½ä½“ç±»å‹
        business_lines: ä¸šåŠ¡çº¿è¿‡æ»¤
        dry_run: æ˜¯å¦é¢„è§ˆæ¨¡å¼
        verbose: æ˜¯å¦è¯¦ç»†è¾“å‡º
        query: æŸ¥è¯¢å…³é”®å­—
        batch_size: æ‰¹å¤„ç†å¤§å°

    Returns:
        æ˜¯å¦å…¨éƒ¨æˆåŠŸ
    """
    logger = logging.getLogger(__name__)
    logger.info("=" * 80)
    logger.info("å¼€å§‹å¯¼å‡ºæ•°æ®åˆ†ææ™ºèƒ½ä½“ç±»å‹æ•°æ®")
    logger.info("=" * 80)

    all_success = True

    # 1. å¯¼å‡ºæ•°æ®è¡¨ï¼ˆPHYSICALï¼‰
    logger.info("\nğŸ“Š [1/2] å¯¼å‡ºæ•°æ®è¡¨...")
    logger.info("-" * 80)
    try:
        table_exporter = DataTableExporter(config, agent_type)
        success = await table_exporter.export(
            business_lines=business_lines,
            dry_run=dry_run,
            verbose=verbose,
            table_type='PHYSICAL',
            query=query,
            batch_size=batch_size
        )
        if not success:
            logger.error("âŒ æ•°æ®è¡¨å¯¼å‡ºå¤±è´¥")
            all_success = False
        else:
            logger.info("âœ… æ•°æ®è¡¨å¯¼å‡ºå®Œæˆ")
    except Exception as e:
        logger.error(f"âŒ æ•°æ®è¡¨å¯¼å‡ºå¼‚å¸¸: {str(e)}", exc_info=True)
        all_success = False

    # 2. å¯¼å‡ºBIæŠ¥è¡¨ï¼ˆREPORTï¼‰
    logger.info("\nğŸ“ˆ [2/2] å¯¼å‡ºBIæŠ¥è¡¨...")
    logger.info("-" * 80)
    try:
        report_exporter = ReportExporter(config, agent_type)
        success = await report_exporter.export(
            business_lines=business_lines,
            dry_run=dry_run,
            verbose=verbose,
            query=query
        )
        if not success:
            logger.error("âŒ BIæŠ¥è¡¨å¯¼å‡ºå¤±è´¥")
            all_success = False
        else:
            logger.info("âœ… BIæŠ¥è¡¨å¯¼å‡ºå®Œæˆ")
    except Exception as e:
        logger.error(f"âŒ BIæŠ¥è¡¨å¯¼å‡ºå¼‚å¸¸: {str(e)}", exc_info=True)
        all_success = False

    return all_success


async def export_data_develop(
    config: ConfigManager,
    agent_type: str,
    business_lines: Optional[List[str]] = None,
    dry_run: bool = False,
    verbose: bool = False,
    batch_size: int = 50
) -> bool:
    """
    å¯¼å‡ºæ•°æ®å¼€å‘æ™ºèƒ½ä½“ç±»å‹çš„æ‰€æœ‰æ•°æ®

    åŒ…æ‹¬ï¼š
    - æ•°æ®æº
    - ä¼ è¾“ä»»åŠ¡

    Args:
        config: é…ç½®ç®¡ç†å™¨
        agent_type: æ™ºèƒ½ä½“ç±»å‹
        business_lines: ä¸šåŠ¡çº¿è¿‡æ»¤
        dry_run: æ˜¯å¦é¢„è§ˆæ¨¡å¼
        verbose: æ˜¯å¦è¯¦ç»†è¾“å‡º
        batch_size: æ‰¹å¤„ç†å¤§å°

    Returns:
        æ˜¯å¦å…¨éƒ¨æˆåŠŸ
    """
    logger = logging.getLogger(__name__)
    logger.info("=" * 80)
    logger.info("å¼€å§‹å¯¼å‡ºæ•°æ®å¼€å‘æ™ºèƒ½ä½“ç±»å‹æ•°æ®")
    logger.info("=" * 80)

    all_success = True

    # 1. å¯¼å‡ºæ•°æ®æº
    logger.info("\nğŸ”Œ [1/2] å¯¼å‡ºæ•°æ®æº...")
    logger.info("-" * 80)
    try:
        datasource_exporter = DataSourceExporter(config, agent_type)
        success = await datasource_exporter.export(
            business_lines=business_lines,
            dry_run=dry_run,
            verbose=verbose,
            batch_size=batch_size
        )
        if not success:
            logger.error("âŒ æ•°æ®æºå¯¼å‡ºå¤±è´¥")
            all_success = False
        else:
            logger.info("âœ… æ•°æ®æºå¯¼å‡ºå®Œæˆ")
    except Exception as e:
        logger.error(f"âŒ æ•°æ®æºå¯¼å‡ºå¼‚å¸¸: {str(e)}", exc_info=True)
        all_success = False

    # 2. å¯¼å‡ºä¼ è¾“ä»»åŠ¡
    logger.info("\nğŸ”„ [2/2] å¯¼å‡ºä¼ è¾“ä»»åŠ¡...")
    logger.info("-" * 80)
    try:
        task_exporter = TransmissionTaskExporter(config, agent_type)
        success = await task_exporter.export(
            business_lines=business_lines,
            dry_run=dry_run,
            verbose=verbose,
            batch_size=batch_size
        )
        if not success:
            logger.error("âŒ ä¼ è¾“ä»»åŠ¡å¯¼å‡ºå¤±è´¥")
            all_success = False
        else:
            logger.info("âœ… ä¼ è¾“ä»»åŠ¡å¯¼å‡ºå®Œæˆ")
    except Exception as e:
        logger.error(f"âŒ ä¼ è¾“ä»»åŠ¡å¯¼å‡ºå¼‚å¸¸: {str(e)}", exc_info=True)
        all_success = False

    return all_success


async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='å¯¼å‡ºæŒ‡å®šæ™ºèƒ½ä½“ç±»å‹çš„æ‰€æœ‰MCPæ•°æ®åˆ°æœ¬åœ°YAMLæ–‡ä»¶',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
æ™ºèƒ½ä½“ç±»å‹è¯´æ˜:
  data_analysis  - æ•°æ®åˆ†ææ™ºèƒ½ä½“ï¼ˆåŒ…å«ï¼šæ•°æ®è¡¨ã€BIæŠ¥è¡¨ï¼‰
  data_develop   - æ•°æ®å¼€å‘æ™ºèƒ½ä½“ï¼ˆåŒ…å«ï¼šæ•°æ®æºã€ä¼ è¾“ä»»åŠ¡ï¼‰

ç¤ºä¾‹:
  # å¯¼å‡ºæ•°æ®åˆ†ææ™ºèƒ½ä½“çš„æ‰€æœ‰æ•°æ®
  %(prog)s --agent-type data_analysis

  # å¯¼å‡ºæ•°æ®å¼€å‘æ™ºèƒ½ä½“çš„iwcä¸šåŠ¡çº¿æ•°æ®
  %(prog)s --agent-type data_develop --business-lines iwc

  # é¢„è§ˆæ¨¡å¼
  %(prog)s --agent-type data_analysis --dry-run
        """
    )
    parser.add_argument(
        '--agent-type',
        required=True,
        choices=['data_analysis', 'data_develop'],
        help='æ™ºèƒ½ä½“ç±»å‹'
    )
    parser.add_argument(
        '--business-lines',
        help='ä¸šåŠ¡çº¿è¿‡æ»¤ï¼Œå¤šä¸ªç”¨é€—å·åˆ†éš”ï¼ˆå¦‚ï¼šiwc,cotï¼‰'
    )
    parser.add_argument(
        '--query',
        help='æŸ¥è¯¢å…³é”®å­—ï¼ˆä»…é€‚ç”¨äºdata_analysisæ™ºèƒ½ä½“ç±»å‹ï¼‰'
    )
    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='é¢„è§ˆæ¨¡å¼ï¼Œä¸å®é™…å†™å…¥æ–‡ä»¶'
    )
    parser.add_argument(
        '--verbose',
        action='store_true',
        help='è¯¦ç»†è¾“å‡º'
    )
    parser.add_argument(
        '--batch-size',
        type=int,
        help='æ‰¹å¤„ç†å¤§å°ï¼ˆdata_analysisé»˜è®¤10ï¼Œdata_developé»˜è®¤50ï¼‰'
    )

    args = parser.parse_args()

    # è®¾ç½®æ—¥å¿—
    setup_logging(args.verbose)
    logger = logging.getLogger(__name__)

    try:
        # éªŒè¯æ™ºèƒ½ä½“ç±»å‹
        agent_type_config = get_agent_type_config(args.agent_type)

        logger.info(f"\n{'=' * 80}")
        logger.info(f"æ™ºèƒ½ä½“ç±»å‹: {args.agent_type}")
        logger.info(f"æè¿°: {agent_type_config['description']}")
        logger.info(f"åŒ…å«æ•°æ®ç±»å‹: {', '.join(agent_type_config['data_types'])}")
        logger.info(f"{'=' * 80}\n")

        # è§£æä¸šåŠ¡çº¿è¿‡æ»¤
        business_lines = None
        if args.business_lines:
            business_lines = [bl.strip() for bl in args.business_lines.split(',')]
            logger.info(f"ä¸šåŠ¡çº¿è¿‡æ»¤: {', '.join(business_lines)}\n")

        # åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨
        config = ConfigManager()

        # éªŒè¯é…ç½®
        is_valid, error_msg = config.validate_config()
        if not is_valid:
            logger.error(f"é…ç½®éªŒè¯å¤±è´¥: {error_msg}")
            sys.exit(1)

        # æ ¹æ®æ™ºèƒ½ä½“ç±»å‹æ‰§è¡Œä¸åŒçš„å¯¼å‡ºé€»è¾‘
        success = False

        if args.agent_type == 'data_analysis':
            # ç¡®å®šæ‰¹å¤„ç†å¤§å°
            batch_size = args.batch_size if args.batch_size is not None else 10

            success = await export_data_analysis(
                config=config,
                agent_type=args.agent_type,
                business_lines=business_lines,
                dry_run=args.dry_run,
                verbose=args.verbose,
                query=args.query,
                batch_size=batch_size
            )
        elif args.agent_type == 'data_develop':
            # queryå‚æ•°å¯¹data_developæ— æ•ˆ
            if args.query:
                logger.warning("âš ï¸  --query å‚æ•°å¯¹ data_develop æ™ºèƒ½ä½“ç±»å‹æ— æ•ˆï¼Œå°†è¢«å¿½ç•¥")

            # ç¡®å®šæ‰¹å¤„ç†å¤§å°
            batch_size = args.batch_size if args.batch_size is not None else 50

            success = await export_data_develop(
                config=config,
                agent_type=args.agent_type,
                business_lines=business_lines,
                dry_run=args.dry_run,
                verbose=args.verbose,
                batch_size=batch_size
            )

        # è¾“å‡ºæœ€ç»ˆç»“æœ
        logger.info("\n" + "=" * 80)
        if success:
            logger.info("âœ… æ™ºèƒ½ä½“ç±»å‹æ•°æ®å¯¼å‡ºä»»åŠ¡å…¨éƒ¨å®Œæˆ")
            logger.info("=" * 80)
            sys.exit(0)
        else:
            logger.error("âŒ éƒ¨åˆ†å¯¼å‡ºä»»åŠ¡å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°é”™è¯¯ä¿¡æ¯")
            logger.info("=" * 80)
            sys.exit(1)

    except KeyboardInterrupt:
        logger.info("\nç”¨æˆ·ä¸­æ–­å¯¼å‡º")
        sys.exit(130)
    except Exception as e:
        logger.error(f"\nå¯¼å‡ºè¿‡ç¨‹å‘ç”Ÿå¼‚å¸¸: {str(e)}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())

"""
TransmissionTask Exporter - ä¼ è¾“ä»»åŠ¡å¯¼å‡ºå™¨
"""
import asyncio
import logging
from datetime import datetime
from typing import List, Optional, Dict, Any, Callable
from pathlib import Path

from .base_exporter import BaseExporter
from ..client.base_mcp_client import BaseMCPClient
from ..config.config_manager import ConfigManager
from ..config.constants import get_mcp_tool_config
from ..utils.file_manager import FileManager
from ..utils.data_processor import (
    build_transmission_task_yaml_data,
    sanitize_filename
)
from ..utils.business_line import extract_business_line

logger = logging.getLogger(__name__)


class TransmissionTaskExporter(BaseExporter):
    """ä¼ è¾“ä»»åŠ¡å¯¼å‡ºå™¨"""

    def __init__(self, config: ConfigManager, agent_type: str = "data_analysis"):
        """
        åˆå§‹åŒ–ä¼ è¾“ä»»åŠ¡å¯¼å‡ºå™¨

        Args:
            config: é…ç½®ç®¡ç†å™¨
            agent_type: æ™ºèƒ½ä½“ç±»å‹
        """
        super().__init__(config, agent_type)

        # åˆå§‹åŒ–æ–‡ä»¶ç®¡ç†å™¨
        self.file_manager = FileManager(str(self.output_dir))

        # è·å–MCPå·¥å…·é…ç½®
        tool_config = get_mcp_tool_config("transmission_task_list")
        service_name = tool_config["service_name"]
        self.tool_function = tool_config["function"]  # ä¿å­˜å®Œæ•´çš„å·¥å…·åç§°

        # åˆå§‹åŒ–MCPå®¢æˆ·ç«¯
        base_url = config.get_mcp_url(service_name)
        self.mcp_client = BaseMCPClient(
            email=config.get("default_email"),
            tenant_id=config.get("tenant_id"),
            base_url=base_url
        )

    async def export(self,
                    business_lines: Optional[List[str]] = None,
                    dry_run: bool = False,
                    verbose: bool = False,
                    batch_size: int = 50) -> bool:
        """
        å¯¼å‡ºä¼ è¾“ä»»åŠ¡

        Args:
            business_lines: ä¸šåŠ¡çº¿è¿‡æ»¤
            dry_run: æ˜¯å¦ä¸ºé¢„è§ˆæ¨¡å¼
            verbose: æ˜¯å¦è¯¦ç»†è¾“å‡º
            batch_size: æ‰¹å¤„ç†å¤§å°

        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        start_time = datetime.now()

        try:
            logger.info("ğŸš€ å¼€å§‹å¯¼å‡ºä¼ è¾“ä»»åŠ¡...")

            if dry_run:
                logger.info("ğŸ“‹ é¢„è§ˆæ¨¡å¼ - ä¸ä¼šå®é™…å†™å…¥æ–‡ä»¶")

            # åˆå§‹åŒ–ç»Ÿè®¡ä¿¡æ¯
            processed_count = 0
            saved_count = 0
            business_line_stats = {}
            saved_files = []
            current_batch = []

            # è¿›åº¦å›è°ƒ
            def update_progress(count: int, name: str):
                nonlocal processed_count
                processed_count = count
                if verbose and (count % 10 == 0 or count <= 5):
                    logger.info(f"ğŸ“Š å·²å¤„ç†: {count} ä¸ªä¼ è¾“ä»»åŠ¡ï¼Œå½“å‰: {name}")

            # æµå¼å¤„ç†ä¼ è¾“ä»»åŠ¡
            async for task in self._stream_transmission_task_list(update_progress):
                try:
                    # ä¸šåŠ¡çº¿è¿‡æ»¤
                    business_line = extract_business_line(task.get("business_segment", ""))
                    if business_lines and business_line not in business_lines:
                        continue

                    # æ„å»ºYAMLæ•°æ®
                    export_time = datetime.now().isoformat()
                    yaml_data = build_transmission_task_yaml_data(
                        task=task,
                        business_line=business_line,
                        export_time=export_time,
                        mcp_tool="mcp__hexin-cbas-data-transmission-mcp__transmission_task_list"
                    )

                    # æ·»åŠ æ–‡ä»¶è·¯å¾„ä¿¡æ¯
                    filename = sanitize_filename(task.get("name", f"task_{processed_count}"))
                    yaml_data["_file_info"] = {
                        "filename": f"{filename}.yaml",
                        "business_line": business_line,
                        "category": "transmission_task"
                    }

                    # æ›´æ–°ä¸šåŠ¡çº¿ç»Ÿè®¡
                    business_line_stats[business_line] = business_line_stats.get(business_line, 0) + 1

                    # æ·»åŠ åˆ°å½“å‰æ‰¹æ¬¡
                    current_batch.append(yaml_data)

                    # æ‰¹é‡å¤„ç†
                    if len(current_batch) >= batch_size:
                        if not dry_run:
                            batch_saved = await self._save_batch_async(current_batch)
                            saved_files.extend(batch_saved)
                            saved_count += len(batch_saved)

                        current_batch.clear()

                        if verbose:
                            logger.info(f"ğŸ’¾ å·²ä¿å­˜æ‰¹æ¬¡ï¼Œç´¯è®¡: {saved_count} ä¸ªæ–‡ä»¶")

                except Exception as e:
                    logger.error(f"å¤„ç†ä¼ è¾“ä»»åŠ¡å¤±è´¥: {str(e)}")
                    continue

            # å¤„ç†å‰©ä½™çš„æ‰¹æ¬¡
            if current_batch:
                if not dry_run:
                    batch_saved = await self._save_batch_async(current_batch)
                    saved_files.extend(batch_saved)
                    saved_count += len(batch_saved)

            # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()

            logger.info("âœ… ä¼ è¾“ä»»åŠ¡å¯¼å‡ºå®Œæˆ!")
            logger.info(f"ğŸ“Š å¤„ç†ä¼ è¾“ä»»åŠ¡: {processed_count} ä¸ª")
            logger.info(f"ğŸ’¾ ä¿å­˜æ–‡ä»¶: {saved_count} ä¸ª")
            logger.info(f"ğŸ“‚ è¾“å‡ºç›®å½•: {self.output_dir}")
            logger.info(f"â±ï¸  è€—æ—¶: {duration:.2f} ç§’")

            if verbose and business_line_stats:
                logger.info("ğŸ“ ä¸šåŠ¡çº¿åˆ†å¸ƒ:")
                for bl, count in sorted(business_line_stats.items()):
                    logger.info(f"  - {bl}: {count} ä¸ª")

            return True

        except Exception as e:
            logger.error(f"ä¼ è¾“ä»»åŠ¡å¯¼å‡ºè¿‡ç¨‹å¼‚å¸¸: {str(e)}")
            return False

        finally:
            await self.mcp_client.close()

    async def _stream_transmission_task_list(self,
                                           progress_callback: Optional[Callable[[int, str], None]] = None):
        """
        æµå¼è·å–ä¼ è¾“ä»»åŠ¡åˆ—è¡¨

        Args:
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°

        Yields:
            å¤„ç†åçš„ä¼ è¾“ä»»åŠ¡ä¿¡æ¯
        """
        # æ„é€ ä¼ è¾“ä»»åŠ¡åˆ—è¡¨çš„MCPè°ƒç”¨å‚æ•°
        arguments = {
            "CBAS_EMAIL": self.config.get("default_email"),
            "x-data-portal-tenant-id": self.config.get("tenant_id")
        }

        processed_count = 0

        def internal_progress_callback(data_item: Dict[str, Any]):
            nonlocal processed_count
            processed_count += 1

            task_name = data_item.get("name", f"task_{processed_count}")

            if progress_callback:
                progress_callback(processed_count, task_name)

        # ä½¿ç”¨åŸºç±»çš„æµå¼è°ƒç”¨æ–¹æ³•ï¼ˆä½¿ç”¨å®Œæ•´çš„å·¥å…·åç§°ï¼‰
        async for task in self.mcp_client.stream_mcp_tool(
            self.tool_function,
            arguments,
            internal_progress_callback
        ):
            yield task

    async def _save_batch_async(self, batch_data: List[Dict[str, Any]]) -> List[str]:
        """
        å¼‚æ­¥æ‰¹é‡ä¿å­˜æ–‡ä»¶

        Args:
            batch_data: æ‰¹é‡æ•°æ®

        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        saved_files = []

        # ä½¿ç”¨å¼‚æ­¥ä»»åŠ¡å¹¶å‘ä¿å­˜æ–‡ä»¶
        tasks = []
        for data in batch_data:
            task = self._save_single_file_async(data)
            tasks.append(task)

        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        results = await asyncio.gather(*tasks, return_exceptions=True)

        for result in results:
            if isinstance(result, str):  # æˆåŠŸä¿å­˜çš„æ–‡ä»¶è·¯å¾„
                saved_files.append(result)
            elif isinstance(result, Exception):
                logger.error(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {str(result)}")

        return saved_files

    async def _save_single_file_async(self, data: Dict[str, Any]) -> Optional[str]:
        """
        å¼‚æ­¥ä¿å­˜å•ä¸ªæ–‡ä»¶

        Args:
            data: ä¼ è¾“ä»»åŠ¡æ•°æ®

        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            file_info = data.get("_file_info")
            if not file_info:
                return None

            filename = file_info["filename"]
            business_line = file_info["business_line"]
            category = file_info["category"]

            file_path = self.file_manager.generate_file_path(filename, business_line, category)

            # åœ¨å¼‚æ­¥ç¯å¢ƒä¸­ä¿å­˜æ–‡ä»¶
            loop = asyncio.get_event_loop()
            success = await loop.run_in_executor(
                None, self.file_manager.save_yaml_file, data, file_path
            )

            if success:
                return str(file_path)

        except Exception as e:
            logger.error(f"å¼‚æ­¥ä¿å­˜æ–‡ä»¶å¤±è´¥: {str(e)}")

        return None

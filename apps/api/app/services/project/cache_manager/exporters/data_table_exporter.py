"""
DataTable Exporter - æ•°æ®è¡¨å¯¼å‡ºå™¨
å…ˆè·å–è¡¨åˆ—è¡¨ï¼Œå†è·å–æ¯ä¸ªè¡¨çš„è¯¦ç»†å­—æ®µä¿¡æ¯
"""
import asyncio
import logging
import os
from datetime import datetime
from typing import List, Optional, Dict, Any
from pathlib import Path

from .base_exporter import BaseExporter
from ..client.easyfetch_https_client import EasyFetchHttpsClient
from ..config.config_manager import ConfigManager
from ..utils.file_manager import FileManager
from ..utils.data_processor import sanitize_filename

logger = logging.getLogger(__name__)


class DataTableExporter(BaseExporter):
    """æ•°æ®è¡¨å¯¼å‡ºå™¨"""

    def __init__(self, config: ConfigManager, agent_type: str = "data_analysis"):
        """
        åˆå§‹åŒ–æ•°æ®è¡¨å¯¼å‡ºå™¨

        Args:
            config: é…ç½®ç®¡ç†å™¨
            agent_type: æ™ºèƒ½ä½“ç±»å‹
        """
        super().__init__(config, agent_type)

        # åˆå§‹åŒ–æ–‡ä»¶ç®¡ç†å™¨
        self.file_manager = FileManager(str(self.output_dir))

        # åˆå§‹åŒ–HTTPSå®¢æˆ·ç«¯ï¼ˆæ›¿ä»£MCPå®¢æˆ·ç«¯ï¼‰
        # æ‰€æœ‰å‚æ•°ä»ç¯å¢ƒå˜é‡è‡ªåŠ¨è¯»å–ï¼Œä¹Ÿå¯ä»¥é€šè¿‡configä¼ å…¥
        self.https_client = EasyFetchHttpsClient(
            email=config.get("default_email"),
            tenant_id=config.get("tenant_id")
            # base_url, cert_path, cert_password è‡ªåŠ¨ä»ç¯å¢ƒå˜é‡è¯»å–
        )

    async def export(self,
                    business_lines: Optional[List[str]] = None,
                    dry_run: bool = False,
                    verbose: bool = False,
                    table_type: str = "PHYSICAL",
                    query: Optional[str] = None,
                    batch_size: int = 10,
                    use_atomic_replace: bool = True) -> bool:
        """
        å¯¼å‡ºæ•°æ®è¡¨schema

        Args:
            business_lines: ä¸šåŠ¡çº¿è¿‡æ»¤
            dry_run: æ˜¯å¦ä¸ºé¢„è§ˆæ¨¡å¼
            verbose: æ˜¯å¦è¯¦ç»†è¾“å‡º
            table_type: è¡¨ç±»å‹ (PHYSICAL/REPORT)
            query: æŸ¥è¯¢å…³é”®å­—
            batch_size: æ‰¹å¤„ç†å¤§å°ï¼ˆè¡¨è¯¦æƒ…æŸ¥è¯¢å¹¶å‘æ•°ï¼‰
            use_atomic_replace: æ˜¯å¦ä½¿ç”¨åŸå­æ€§æ›¿æ¢ï¼ˆé»˜è®¤Trueï¼‰

        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        start_time = datetime.now()

        try:
            logger.info(f"ğŸš€ å¼€å§‹å¯¼å‡ºæ•°æ®è¡¨schema (ç±»å‹: {table_type})...")

            if dry_run:
                logger.info("ğŸ“‹ é¢„è§ˆæ¨¡å¼ - ä¸ä¼šå®é™…å†™å…¥æ–‡ä»¶")

            if use_atomic_replace and not dry_run:
                logger.info("ğŸ”„ ä½¿ç”¨åŸå­æ€§æ›¿æ¢æ¨¡å¼ï¼ˆå…ˆå†™å…¥ä¸´æ—¶ç›®å½•ï¼Œå®ŒæˆååŸå­æ€§æ›¿æ¢ï¼‰")

            # ç¬¬ä¸€æ­¥ï¼šè·å–è¡¨åˆ—è¡¨
            logger.info("ğŸ“¡ æ­£åœ¨è·å–è¡¨åˆ—è¡¨...")
            table_list = await self._get_table_list(table_type, query)

            if not table_list:
                logger.warning("æœªæ‰¾åˆ°ä»»ä½•è¡¨")
                return True

            logger.info(f"ğŸ“Š æ‰¾åˆ° {len(table_list)} ä¸ªè¡¨")

            # åˆå§‹åŒ–ç»Ÿè®¡ä¿¡æ¯
            processed_count = 0
            saved_count = 0
            business_line_stats = {}
            saved_files = []
            affected_business_lines = set()  # è®°å½•å—å½±å“çš„ä¸šåŠ¡çº¿

            # ç¬¬äºŒæ­¥ï¼šæ‰¹é‡è·å–è¡¨è¯¦æƒ…å¹¶ä¿å­˜
            for i in range(0, len(table_list), batch_size):
                batch = table_list[i:i+batch_size]

                if verbose:
                    logger.info(f"ğŸ“¥ æ­£åœ¨è·å–è¡¨è¯¦æƒ… ({i+1}-{min(i+batch_size, len(table_list))}/{len(table_list)})...")

                # å¹¶å‘è·å–è¡¨è¯¦æƒ…
                detail_tasks = [
                    self._get_table_detail(str(table["id"]))
                    for table in batch
                ]
                details = await asyncio.gather(*detail_tasks, return_exceptions=True)

                # å¤„ç†æ¯ä¸ªè¡¨çš„è¯¦æƒ…
                for table_meta, detail in zip(batch, details):
                    try:
                        if isinstance(detail, Exception):
                            logger.error(f"è·å–è¡¨è¯¦æƒ…å¤±è´¥ (ID: {table_meta.get('id')}): {str(detail)}")
                            continue

                        # åˆå¹¶è¡¨å…ƒæ•°æ®å’Œè¯¦æƒ…
                        merged_data = self._merge_table_data(table_meta, detail)

                        # è¿‡æ»¤æ¡ä»¶ï¼šåªå¯¼å‡º enable_tag=true ä¸” datasource_name=presto-hive çš„è¡¨
                        enable_tag = merged_data.get("enable_tag", False)
                        datasource_name = merged_data.get("datasource_name", "")

                        if not enable_tag or datasource_name != "presto-hive":
                            if verbose:
                                table_name = merged_data.get("en_name", "unknown")
                                logger.debug(f"è·³è¿‡è¡¨ {table_name}: enable_tag={enable_tag}, datasource_name={datasource_name}")
                            continue

                        # æå–ä¸šåŠ¡çº¿ï¼ˆæ”¯æŒå¤šä¸ªä¸šåŠ¡çº¿ï¼‰
                        business_lines_list = merged_data.get("business_line", [])
                        if not business_lines_list:
                            business_lines_list = ["unknown"]

                        # ç‰¹æ®Šé€»è¾‘ï¼šå°† "dream" æ›¿æ¢ä¸º "dreamface"
                        business_lines_list = [
                            "dreamface" if bl == "dream" else bl
                            for bl in business_lines_list
                        ]

                        # ä¸šåŠ¡çº¿è¿‡æ»¤
                        if business_lines:
                            business_lines_list = [
                                bl for bl in business_lines_list if bl in business_lines
                            ]
                            if not business_lines_list:
                                continue

                        processed_count += 1

                        # ä¸ºæ¯ä¸ªä¸šåŠ¡çº¿ä¿å­˜ä¸€ä»½æ–‡ä»¶
                        for business_line in business_lines_list:
                            # æ„å»ºYAMLæ•°æ®ï¼ˆä¼ å…¥æ›¿æ¢åçš„ä¸šåŠ¡çº¿ï¼‰
                            yaml_data = self._build_yaml_data(merged_data, business_line)

                            # æ·»åŠ æ–‡ä»¶è·¯å¾„ä¿¡æ¯
                            table_name = merged_data.get("en_name") or merged_data.get("name", f"table_{processed_count}")
                            filename = sanitize_filename(table_name)
                            yaml_data["_file_info"] = {
                                "filename": f"{filename}.yaml",
                                "business_line": business_line,
                                "category": "data_table",
                                "use_temp_dir": use_atomic_replace  # æ ‡è®°æ˜¯å¦ä½¿ç”¨ä¸´æ—¶ç›®å½•
                            }

                            # æ›´æ–°ä¸šåŠ¡çº¿ç»Ÿè®¡
                            business_line_stats[business_line] = business_line_stats.get(business_line, 0) + 1
                            affected_business_lines.add(business_line)  # è®°å½•å—å½±å“çš„ä¸šåŠ¡çº¿

                            # ä¿å­˜æ–‡ä»¶
                            if not dry_run:
                                file_path = await self._save_single_file_async(yaml_data, use_atomic_replace)
                                if file_path:
                                    saved_files.append(file_path)
                                    saved_count += 1

                        if verbose and processed_count % 10 == 0:
                            logger.info(f"ğŸ“Š å·²å¤„ç†: {processed_count} ä¸ªè¡¨")

                    except Exception as e:
                        logger.error(f"å¤„ç†è¡¨å¤±è´¥ (ID: {table_meta.get('id')}): {str(e)}")
                        continue

            # åŸå­æ€§æ›¿æ¢ï¼šå°†ä¸´æ—¶ç›®å½•æ›¿æ¢ä¸ºæ­£å¼ç›®å½•
            if use_atomic_replace and not dry_run and affected_business_lines:
                logger.info("\nğŸ”„ å¼€å§‹åŸå­æ€§æ›¿æ¢ç›®å½•...")
                replace_success_count = 0
                replace_fail_count = 0

                for business_line in sorted(affected_business_lines):
                    try:
                        success = self.file_manager.atomic_replace_category_dir(
                            business_line=business_line,
                            category="data_table"
                        )
                        if success:
                            replace_success_count += 1
                        else:
                            replace_fail_count += 1
                    except Exception as e:
                        logger.error(f"åŸå­æ€§æ›¿æ¢å¤±è´¥ {business_line}/data_table: {e}")
                        replace_fail_count += 1

                logger.info(f"âœ… åŸå­æ€§æ›¿æ¢å®Œæˆ: æˆåŠŸ {replace_success_count} ä¸ªï¼Œå¤±è´¥ {replace_fail_count} ä¸ª")

                if replace_fail_count > 0:
                    logger.warning("éƒ¨åˆ†ä¸šåŠ¡çº¿çš„åŸå­æ€§æ›¿æ¢å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")

            # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()

            logger.info("\n" + "=" * 80)
            logger.info("âœ… æ•°æ®è¡¨schemaå¯¼å‡ºå®Œæˆ!")
            logger.info(f"ğŸ“Š å¤„ç†è¡¨: {processed_count} ä¸ª")
            logger.info(f"ğŸ’¾ ä¿å­˜æ–‡ä»¶: {saved_count} ä¸ª")
            logger.info(f"ğŸ“‚ è¾“å‡ºç›®å½•: {self.output_dir}")
            logger.info(f"â±ï¸  è€—æ—¶: {duration:.2f} ç§’")

            if verbose and business_line_stats:
                logger.info("\nğŸ“ ä¸šåŠ¡çº¿åˆ†å¸ƒ:")
                for bl, count in sorted(business_line_stats.items()):
                    logger.info(f"  - {bl}: {count} ä¸ª")

            logger.info("=" * 80)
            return True

        except Exception as e:
            logger.error(f"æ•°æ®è¡¨schemaå¯¼å‡ºè¿‡ç¨‹å¼‚å¸¸: {str(e)}", exc_info=True)
            return False

        finally:
            await self.https_client.close()

    async def _get_table_list(self, table_type: str, query: Optional[str] = None) -> List[Dict[str, Any]]:
        """
        è·å–è¡¨åˆ—è¡¨ï¼ˆé€šè¿‡HTTPSæ¥å£ï¼‰

        Args:
            table_type: è¡¨ç±»å‹
            query: æŸ¥è¯¢å…³é”®å­—

        Returns:
            è¡¨åˆ—è¡¨
        """
        try:
            # è°ƒç”¨HTTPSæ¥å£è·å–è¡¨åˆ—è¡¨
            result = await self.https_client.get_table_list(
                query=query or "",
                table_type=table_type
            )

            # è°ƒè¯•:æ‰“å°è¿”å›ç»“æœç»“æ„
            logger.debug(f"HTTPSè¿”å›ç»“æœç±»å‹: {type(result)}")
            logger.debug(f"HTTPSè¿”å›ç»“æœkeys: {result.keys() if isinstance(result, dict) else 'N/A'}")

            # æ£€æŸ¥å“åº”çŠ¶æ€
            if result.get("status_code") != 0:
                error_msg = result.get("status_msg", "Unknown error")
                logger.error(f"è·å–è¡¨åˆ—è¡¨å¤±è´¥: {error_msg}")
                return []

            # ä»ç»“æœçš„dataå­—æ®µä¸­æå–è¡¨åˆ—è¡¨
            data = result.get("data", {})
            tables = self._extract_tables_from_tree(data)
            return tables

        except Exception as e:
            logger.error(f"è·å–è¡¨åˆ—è¡¨å¤±è´¥: {str(e)}")
            raise

    def _extract_tables_from_tree(self, data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        ä»æ ‘ç»“æ„ä¸­æå–è¡¨åˆ—è¡¨

        æ ‘ç»“æ„ä½¿ç”¨nextå­—æ®µè¡¨ç¤ºå­å…ƒç´ ï¼Œé€šè¿‡type=tableè¯†åˆ«æ•°æ®è¡¨

        Args:
            data: æ ‘ç»“æ„æ•°æ®ï¼ˆæ¥è‡ªHTTPSæ¥å£çš„dataå­—æ®µï¼‰

        Returns:
            è¡¨åˆ—è¡¨
        """
        tables = []

        def traverse(node: Any, path: str = "") -> None:
            """é€’å½’éå†æ ‘èŠ‚ç‚¹"""
            if not isinstance(node, dict):
                return

            # åˆ¤æ–­æ˜¯å¦ä¸ºè¡¨èŠ‚ç‚¹
            node_type = node.get("type", "")
            if node_type == "table":
                # æå–è¡¨ä¿¡æ¯
                node_data = node.get("data", {})
                tables.append({
                    "id": node_data.get("id"),
                    "name": node_data.get("cn_name") or node_data.get("en_name") or node.get("name", ""),
                    "en_name": node_data.get("en_name", ""),
                    "cn_name": node_data.get("cn_name", ""),
                    "path": path,
                    "type": node_type,
                    "business_line": node_data.get("business_line", []),
                    "description": node_data.get("description") or node_data.get("extra_info", {}).get("description", ""),
                    # ä¿å­˜è¡¨çš„å…ƒæ•°æ®ä¿¡æ¯
                    "metadata": node_data
                })

            # éå†å­èŠ‚ç‚¹ï¼ˆnextå­—æ®µï¼‰
            next_nodes = node.get("next")
            if next_nodes:
                # æ›´æ–°è·¯å¾„
                node_name = node.get("name", "")
                new_path = f"{path}/{node_name}" if path else node_name

                # nextå¯èƒ½æ˜¯åˆ—è¡¨æˆ–å•ä¸ªå¯¹è±¡
                if isinstance(next_nodes, list):
                    for child in next_nodes:
                        traverse(child, new_path)
                elif isinstance(next_nodes, dict):
                    traverse(next_nodes, new_path)

        # ä»rootèŠ‚ç‚¹å¼€å§‹éå†
        root = data.get("root", {})
        if root:
            traverse(root)

        return tables

    async def _get_table_detail(self, table_id: str) -> Dict[str, Any]:
        """
        è·å–è¡¨è¯¦æƒ…ï¼ˆé€šè¿‡HTTPSæ¥å£ï¼‰

        Args:
            table_id: è¡¨ID

        Returns:
            è¡¨è¯¦æƒ…
        """
        try:
            # è°ƒç”¨HTTPSæ¥å£è·å–è¡¨è¯¦æƒ…
            result = await self.https_client.get_table_detail(table_id)

            # æ£€æŸ¥å“åº”çŠ¶æ€
            if result.get("status_code") != 0:
                error_msg = result.get("status_msg", "Unknown error")
                logger.error(f"è·å–è¡¨è¯¦æƒ…å¤±è´¥ (ID: {table_id}): {error_msg}")
                return {}

            # è¿”å›dataå­—æ®µ
            return result.get("data", {})

        except Exception as e:
            logger.error(f"è·å–è¡¨è¯¦æƒ…å¤±è´¥ (ID: {table_id}): {str(e)}")
            raise

    def _merge_table_data(self, table_meta: Dict[str, Any], table_detail: Dict[str, Any]) -> Dict[str, Any]:
        """
        åˆå¹¶è¡¨å…ƒæ•°æ®å’Œè¯¦æƒ…

        Args:
            table_meta: è¡¨å…ƒæ•°æ®ï¼ˆä»ç›®å½•æ ‘è·å–ï¼‰
            table_detail: è¡¨è¯¦æƒ…ï¼ˆä»HTTPSæ¥å£è·å–ï¼Œå·²ç»æ˜¯dataå­—æ®µï¼‰

        Returns:
            åˆå¹¶åçš„æ•°æ®
        """
        # HTTPSæ¥å£ç›´æ¥è¿”å›çš„æ˜¯dataå†…å®¹ï¼Œä¸éœ€è¦é¢å¤–è§£æ
        detail_data = table_detail

        # åˆå¹¶æ•°æ®ï¼Œdetailä¼˜å…ˆ
        merged = {
            "id": detail_data.get("id") or table_meta.get("id"),
            "en_name": detail_data.get("en_name") or table_meta.get("en_name", ""),
            "cn_name": detail_data.get("cn_name") or table_meta.get("cn_name", ""),
            "standard_name": detail_data.get("standard_name", ""),
            "business_line": detail_data.get("business_line") or table_meta.get("business_line", []),
            "datasource_name": detail_data.get("datasource_name", ""),
            "db_name": detail_data.get("db_name", ""),
            "table_type": detail_data.get("table_type") or table_meta.get("type", ""),
            "lifecycle": detail_data.get("lifecycle"),
            "description": detail_data.get("extra_info", {}).get("description") or table_meta.get("description", ""),
            "path": table_meta.get("path", ""),
            "dir_id": detail_data.get("dir_id"),
            "primary_column_list": detail_data.get("primary_column_list", []),
            "partition_column_list": detail_data.get("partition_column_list", []),
            "column_list": detail_data.get("column_list", []),
            "extra_info": detail_data.get("extra_info", {}),
            "enable_tag": detail_data.get("enable_tag", True),
            "create_time": detail_data.get("create_time", ""),
            "update_time": detail_data.get("update_time"),
            "creator": detail_data.get("creator", ""),
            "modifier": detail_data.get("modifier"),
            "principal": detail_data.get("principal", ""),
            "uuid": detail_data.get("uuid", ""),
        }

        return merged

    def _build_yaml_data(self, table_data: Dict[str, Any], business_line: str) -> Dict[str, Any]:
        """
        æ„å»ºYAMLæ•°æ®

        Args:
            table_data: è¡¨æ•°æ®
            business_line: ä¸šåŠ¡çº¿

        Returns:
            YAMLæ•°æ®
        """
        import json

        # å¤„ç† extra_info å­—æ®µï¼šå¦‚æœæ˜¯å­—å…¸ä¸”éç©ºï¼Œè½¬æ¢ä¸ºæ ¼å¼åŒ–çš„ JSON å­—ç¬¦ä¸²ï¼ˆæ ‡å‡†æ ¼å¼ï¼Œä¸å«æ³¨é‡Šï¼‰
        extra_info = table_data.get("extra_info", {})
        if isinstance(extra_info, dict) and extra_info:
            # ç§»é™¤ env å­—æ®µ
            extra_info = extra_info.copy()
            extra_info.pop("env", None)
            # æ ¼å¼åŒ–ä¸ºå¸¦ç¼©è¿›çš„æ ‡å‡† JSON å­—ç¬¦ä¸²
            extra_info = json.dumps(extra_info, indent=2, ensure_ascii=False)
        elif not extra_info:
            # å¦‚æœä¸ºç©ºï¼Œä¿æŒä¸ºç©ºå­—å…¸
            extra_info = {}

        # å¤„ç† columns å­—æ®µï¼šç®€åŒ–ä¸ºåªåŒ…å«æ ¸å¿ƒå­—æ®µçš„åˆ—è¡¨ï¼Œä½¿ç”¨JSONæ ¼å¼ï¼ˆä¸bi_reportä¿æŒä¸€è‡´ï¼‰
        columns = table_data.get("column_list", [])

        if columns:
            # ç®€åŒ–æ¯ä¸ªå­—æ®µï¼Œåªä¿ç•™æ ¸å¿ƒä¿¡æ¯ï¼ˆå»æ‰order_noå’Œdata_formatï¼‰
            simplified_columns = []
            for col in columns:
                simplified_col = {
                    "en_name": col.get("en_name", ""),
                    "cn_name": col.get("cn_name", ""),
                    "type": col.get("type", ""),
                    "sample_data": col.get("sample_data")
                }

                # å¦‚æœè¯¥åˆ—æœ‰å­—å…¸ä¿¡æ¯ï¼Œæ·»åŠ åˆ°å­—æ®µä¸­
                dict_info = col.get("dict")
                if dict_info and isinstance(dict_info, dict):
                    simplified_col["dict"] = dict_info

                simplified_columns.append(simplified_col)

            # ä½¿ç”¨json.dumpsæ ¼å¼åŒ–ä¸ºå¤šè¡ŒJSONå­—ç¬¦ä¸²
            json_str = json.dumps(simplified_columns, indent=2, ensure_ascii=False)
            # ä½¿ç”¨ç‰¹æ®Šæ ‡è®°ï¼Œåç»­å¤„ç†æ—¶è½¬æ¢ä¸ºliteral block scalar
            columns_display = f"__LITERAL_BLOCK_START__\n{json_str}\n__LITERAL_BLOCK_END__"
        else:
            columns_display = "[]"

        # å¤„ç† business_lineï¼šä½¿ç”¨ä¼ å…¥çš„ business_line å‚æ•°ï¼ˆå·²ç»è¿‡ç‰¹æ®Šé€»è¾‘å¤„ç†ï¼‰
        # å°†å•ä¸ªä¸šåŠ¡çº¿è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
        business_line_list = [business_line] if business_line else table_data.get("business_line", [])

        # æŒ‰æŒ‡å®šé¡ºåºè¿”å›å­—æ®µ
        yaml_data = {
            "id": table_data.get("id", ""),
            "en_name": table_data.get("en_name", ""),
            "cn_name": table_data.get("cn_name", ""),
            "db_name": table_data.get("db_name", ""),
            "business_line": business_line_list,
            "datasource_name": table_data.get("datasource_name", ""),
            "table_type": table_data.get("table_type", ""),
            "primary_column_list": table_data.get("primary_column_list", []),
            "partition_column_list": table_data.get("partition_column_list", []),
            "extra_info": extra_info,
            "columns": columns_display
        }

        # æ·»åŠ å­—æ®µæ³¨é‡Š
        yaml_data["_field_comments"] = {
            "en_name": "è¡¨å",
            "db_name": "æ•°æ®åº“åç§°",
            "business_line": "å½’å±ä¸šåŠ¡çº¿",
            "datasource_name": "è¡¨æ‰€åœ¨å­˜å‚¨å¼•æ“",
            "primary_column_list": "ä¸»é”®å­—æ®µ",
            "partition_column_list": "åˆ†åŒºå­—æ®µ",
            "extra_info": "æ‰©å±•ä¿¡æ¯(description:è¡¨æè¿°ä¿¡æ¯, foreign_condition:å¤–è¡¨å…³è”å…³ç³»)",
            "columns": "è¡¨å­—æ®µåˆ—è¡¨"
        }

        return yaml_data

    async def _save_single_file_async(self, data: Dict[str, Any], use_temp_dir: bool = False) -> Optional[str]:
        """
        å¼‚æ­¥ä¿å­˜å•ä¸ªæ–‡ä»¶

        Args:
            data: è¡¨æ•°æ®
            use_temp_dir: æ˜¯å¦ä½¿ç”¨ä¸´æ—¶ç›®å½•ï¼ˆç”¨äºåŸå­æ€§æ›¿æ¢ï¼‰

        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            file_info = data.get("_file_info")
            if not file_info:
                return None

            filename = file_info["filename"]
            business_line = file_info["business_line"]
            category = file_info["category"]

            # æ ¹æ®æ˜¯å¦ä½¿ç”¨ä¸´æ—¶ç›®å½•é€‰æ‹©ä¸åŒçš„è·¯å¾„ç”Ÿæˆæ–¹æ³•
            if use_temp_dir:
                file_path = self.file_manager.generate_file_path_in_temp(filename, business_line, category)
            else:
                file_path = self.file_manager.generate_file_path(filename, business_line, category)

            # åœ¨å¼‚æ­¥ç¯å¢ƒä¸­ä¿å­˜æ–‡ä»¶
            loop = asyncio.get_event_loop()
            success = await loop.run_in_executor(
                None, self.file_manager.save_yaml_file, data, file_path
            )

            if success:
                return str(file_path)

        except Exception as e:
            logger.error(f"å¼‚æ­¥ä¿å­˜æ–‡ä»¶å¤±è´¥: {str(e)}")

        return None

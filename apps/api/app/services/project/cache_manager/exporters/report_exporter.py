"""
Report Exporter - æŠ¥è¡¨å¯¼å‡ºå™¨
"""
import asyncio
import logging
import os
from datetime import datetime
from typing import List, Optional, Dict, Any, Callable
from pathlib import Path

from .base_exporter import BaseExporter
from ..client.easyfetch_https_client import EasyFetchHttpsClient
from ..config.config_manager import ConfigManager
from ..utils.file_manager import FileManager
from ..utils.data_processor import sanitize_filename

logger = logging.getLogger(__name__)


class ReportExporter(BaseExporter):
    """æŠ¥è¡¨å¯¼å‡ºå™¨"""

    def __init__(self, config: ConfigManager, agent_type: str = "data_analysis"):
        """
        åˆå§‹åŒ–æŠ¥è¡¨å¯¼å‡ºå™¨

        Args:
            config: é…ç½®ç®¡ç†å™¨
            agent_type: æ™ºèƒ½ä½“ç±»å‹
        """
        super().__init__(config, agent_type)

        # åˆå§‹åŒ–æ–‡ä»¶ç®¡ç†å™¨
        self.file_manager = FileManager(str(self.output_dir))

        # åˆå§‹åŒ–HTTPSå®¢æˆ·ç«¯ï¼ˆæ›¿ä»£MCPå®¢æˆ·ç«¯ï¼‰
        # æ‰€æœ‰å‚æ•°ä»ç¯å¢ƒå˜é‡è‡ªåŠ¨è¯»å–ï¼Œä¹Ÿå¯ä»¥é€šè¿‡configä¼ å…¥
        self.https_client = EasyFetchHttpsClient(
            email=config.get("default_email"),
            tenant_id=config.get("tenant_id")
            # base_url, cert_path, cert_password è‡ªåŠ¨ä»ç¯å¢ƒå˜é‡è¯»å–
        )

    async def export(self,
                    business_lines: Optional[List[str]] = None,
                    dry_run: bool = False,
                    verbose: bool = False,
                    query: Optional[str] = None,
                    use_atomic_replace: bool = True) -> bool:
        """
        å¯¼å‡ºæŠ¥è¡¨

        Args:
            business_lines: ä¸šåŠ¡çº¿è¿‡æ»¤
            dry_run: æ˜¯å¦ä¸ºé¢„è§ˆæ¨¡å¼
            verbose: æ˜¯å¦è¯¦ç»†è¾“å‡º
            query: æŸ¥è¯¢å…³é”®å­—
            use_atomic_replace: æ˜¯å¦ä½¿ç”¨åŸå­æ€§æ›¿æ¢ï¼ˆé»˜è®¤Trueï¼‰

        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        start_time = datetime.now()

        try:
            logger.info("ğŸš€ å¼€å§‹å¯¼å‡ºæŠ¥è¡¨...")

            if dry_run:
                logger.info("ğŸ“‹ é¢„è§ˆæ¨¡å¼ - ä¸ä¼šå®é™…å†™å…¥æ–‡ä»¶")

            if use_atomic_replace and not dry_run:
                logger.info("ğŸ”„ ä½¿ç”¨åŸå­æ€§æ›¿æ¢æ¨¡å¼ï¼ˆå…ˆå†™å…¥ä¸´æ—¶ç›®å½•ï¼Œå®ŒæˆååŸå­æ€§æ›¿æ¢ï¼‰")

            # åˆå§‹åŒ–ç»Ÿè®¡ä¿¡æ¯
            processed_count = 0
            saved_count = 0
            business_line_stats = {}
            saved_files = []
            affected_business_lines = set()  # è®°å½•å—å½±å“çš„ä¸šåŠ¡çº¿

            # è·å–æŠ¥è¡¨ç›®å½•æ ‘
            tree_data = await self._fetch_report_tree(query)
            if not tree_data:
                logger.error("è·å–æŠ¥è¡¨ç›®å½•æ ‘å¤±è´¥")
                return False

            # éå†ç›®å½•æ ‘ï¼Œæå–æ‰€æœ‰æŠ¥è¡¨
            root_node = tree_data.get("root")
            if not root_node:
                logger.error("æŠ¥è¡¨ç›®å½•æ ‘æ ¹èŠ‚ç‚¹ä¸å­˜åœ¨")
                return False

            # é€’å½’å¤„ç†å­èŠ‚ç‚¹
            async def process_node(node: Dict[str, Any], path_parts: List[str]):
                nonlocal processed_count, saved_count

                # è·å–èŠ‚ç‚¹ä¿¡æ¯
                node_id = node.get("id")
                node_name = node.get("name")
                node_type = node.get("type")
                unique_id = node.get("unique_id")
                parent_unique_id = node.get("parent_unique_id")
                sub_node_list = node.get("sub_node_list", [])

                # å¦‚æœæ˜¯æŠ¥è¡¨èŠ‚ç‚¹ï¼ˆtype="REPORT"ï¼‰
                if node_type == "REPORT":
                    processed_count += 1

                    # æ„å»ºè·¯å¾„ï¼šä¸šåŠ¡çº¿/æ•°æ®ç±»å‹/å­ç›®å½•/æŠ¥è¡¨åç§°
                    # path_parts[0] æ˜¯ä¸šåŠ¡çº¿
                    # path_parts[1] æ˜¯æ•°æ®ç±»å‹ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                    # path_parts[2:] æ˜¯å­ç›®å½•
                    business_line = path_parts[0] if len(path_parts) > 0 else "unknown"

                    # ç‰¹æ®Šé€»è¾‘ï¼šå°† "dream" æ›¿æ¢ä¸º "dreamface"
                    if business_line == "dream":
                        business_line = "dreamface"

                    # ä¸šåŠ¡çº¿è¿‡æ»¤
                    if business_lines and business_line not in business_lines:
                        return

                    if verbose:
                        logger.info(f"ğŸ“Š å¤„ç†æŠ¥è¡¨ [{processed_count}]: {'/'.join(path_parts + [node_name])}")

                    # è·å–æŠ¥è¡¨è¯¦æƒ…
                    report_detail = await self._fetch_report_detail(node_id)
                    if not report_detail:
                        logger.warning(f"è·å–æŠ¥è¡¨è¯¦æƒ…å¤±è´¥: {node_name} (ID: {node_id})")
                        return

                    # è¿‡æ»¤æ¡ä»¶ï¼šåªå¯¼å‡º enable_tag=true çš„æŠ¥è¡¨
                    enable_tag = report_detail.get("enable_tag", False)
                    if not enable_tag:
                        if verbose:
                            logger.debug(f"è·³è¿‡æŠ¥è¡¨ {node_name}: enable_tag={enable_tag}")
                        return

                    # æ„å»ºYAMLæ•°æ®
                    export_time = datetime.now().isoformat()
                    yaml_data = self._build_report_yaml_data(
                        node=node,
                        report_detail=report_detail,
                        path_parts=path_parts,
                        export_time=export_time
                    )

                    # æ›´æ–°ä¸šåŠ¡çº¿ç»Ÿè®¡
                    business_line_stats[business_line] = business_line_stats.get(business_line, 0) + 1
                    affected_business_lines.add(business_line)  # è®°å½•å—å½±å“çš„ä¸šåŠ¡çº¿

                    # ä¿å­˜æ–‡ä»¶
                    if not dry_run:
                        # æŠ¥è¡¨çš„æ•°æ®ç±»å‹å›ºå®šä¸º"bi_report"
                        category = "bi_report"

                        # å­ç›®å½•è·¯å¾„ï¼špath_partsä»ç¬¬2ä¸ªå…ƒç´ å¼€å§‹ï¼ˆç¬¬1ä¸ªæ˜¯ä¸šåŠ¡çº¿ï¼‰
                        # ä¾‹å¦‚ï¼špath_parts = ["cot", "Aè‚¡å¼€æˆ·2.0 --åˆ†å…¥å£"]
                        # sub_dirs = ["Aè‚¡å¼€æˆ·2.0 --åˆ†å…¥å£"]
                        sub_dirs = path_parts[1:] if len(path_parts) > 1 else []

                        # ç”Ÿæˆæ–‡ä»¶å
                        filename = sanitize_filename(node_name)

                        # æ‰‹åŠ¨æ„å»ºå®Œæ•´è·¯å¾„ï¼šä½¿ç”¨ä¸´æ—¶ç›®å½•æˆ–æ­£å¼ç›®å½•
                        if use_atomic_replace:
                            file_path = self.file_manager.get_temp_category_path(business_line, category)
                        else:
                            file_path = self.output_dir / business_line / category

                        for sub_dir in sub_dirs:
                            file_path = file_path / sanitize_filename(sub_dir)
                        file_path = file_path / f"{filename}.yaml"

                        # ä¿å­˜æ–‡ä»¶ï¼ˆæŠ¥è¡¨ä½¿ç”¨è‡ªå®šä¹‰ä¿å­˜æ–¹æ³•ï¼‰
                        success = self._save_report_yaml_file(yaml_data, file_path)
                        if success:
                            saved_count += 1
                            saved_files.append(str(file_path))

                            if verbose:
                                logger.info(f"ğŸ’¾ å·²ä¿å­˜: {file_path}")

                # å¦‚æœæœ‰å­èŠ‚ç‚¹ï¼Œé€’å½’å¤„ç†
                elif sub_node_list:
                    # æ„å»ºæ–°çš„è·¯å¾„
                    new_path = path_parts + [node_name] if node_name != "REPORT" else path_parts

                    for sub_node in sub_node_list:
                        await process_node(sub_node, new_path)

            # å¤„ç†æ ¹èŠ‚ç‚¹ä¸‹çš„æ‰€æœ‰ä¸šåŠ¡çº¿
            sub_nodes = root_node.get("sub_node_list", [])
            for business_line_node in sub_nodes:
                await process_node(business_line_node, [])

            # åŸå­æ€§æ›¿æ¢ï¼šå°†ä¸´æ—¶ç›®å½•æ›¿æ¢ä¸ºæ­£å¼ç›®å½•
            if use_atomic_replace and not dry_run and affected_business_lines:
                logger.info("\nğŸ”„ å¼€å§‹åŸå­æ€§æ›¿æ¢ç›®å½•...")
                replace_success_count = 0
                replace_fail_count = 0

                for business_line in sorted(affected_business_lines):
                    try:
                        success = self.file_manager.atomic_replace_category_dir(
                            business_line=business_line,
                            category="bi_report"
                        )
                        if success:
                            replace_success_count += 1
                        else:
                            replace_fail_count += 1
                    except Exception as e:
                        logger.error(f"åŸå­æ€§æ›¿æ¢å¤±è´¥ {business_line}/bi_report: {e}")
                        replace_fail_count += 1

                logger.info(f"âœ… åŸå­æ€§æ›¿æ¢å®Œæˆ: æˆåŠŸ {replace_success_count} ä¸ªï¼Œå¤±è´¥ {replace_fail_count} ä¸ª")

                if replace_fail_count > 0:
                    logger.warning("éƒ¨åˆ†ä¸šåŠ¡çº¿çš„åŸå­æ€§æ›¿æ¢å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")

            # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()

            logger.info("\n" + "=" * 80)
            logger.info("âœ… æŠ¥è¡¨å¯¼å‡ºå®Œæˆ!")
            logger.info(f"ğŸ“Š å¤„ç†æŠ¥è¡¨: {processed_count} ä¸ª")
            logger.info(f"ğŸ’¾ ä¿å­˜æ–‡ä»¶: {saved_count} ä¸ª")
            logger.info(f"ğŸ“‚ è¾“å‡ºç›®å½•: {self.output_dir}")
            logger.info(f"â±ï¸  è€—æ—¶: {duration:.2f} ç§’")

            if verbose and business_line_stats:
                logger.info("\nğŸ“ ä¸šåŠ¡çº¿åˆ†å¸ƒ:")
                for bl, count in sorted(business_line_stats.items()):
                    logger.info(f"  - {bl}: {count} ä¸ª")

            logger.info("=" * 80)
            return True

        except Exception as e:
            logger.error(f"æŠ¥è¡¨å¯¼å‡ºè¿‡ç¨‹å¼‚å¸¸: {str(e)}", exc_info=True)
            return False

        finally:
            await self.https_client.close()

    async def _fetch_report_tree(self, query: Optional[str] = None) -> Optional[Dict[str, Any]]:
        """
        è·å–æŠ¥è¡¨ç›®å½•æ ‘ï¼ˆé€šè¿‡HTTPSæ¥å£ï¼‰

        Args:
            query: æŸ¥è¯¢å…³é”®å­—

        Returns:
            æŠ¥è¡¨ç›®å½•æ ‘æ•°æ®
        """
        try:
            # è°ƒç”¨HTTPSæ¥å£è·å–æŠ¥è¡¨åˆ—è¡¨
            result = await self.https_client.get_report_list(
                query=query or "",
                table_type="REPORT"
            )

            # è°ƒè¯•ï¼šæ‰“å°è¿”å›ç»“æœç»“æ„
            logger.debug(f"HTTPSè¿”å›ç»“æœç±»å‹: {type(result)}")
            logger.debug(f"HTTPSè¿”å›ç»“æœkeys: {result.keys() if isinstance(result, dict) else 'N/A'}")

            # æ£€æŸ¥å“åº”çŠ¶æ€
            if result.get("status_code") == 0:
                return result.get("data")

            error_msg = result.get("status_msg", "Unknown error")
            logger.error(f"è·å–æŠ¥è¡¨ç›®å½•æ ‘å¤±è´¥: {error_msg}")
            return None

        except Exception as e:
            logger.error(f"è·å–æŠ¥è¡¨ç›®å½•æ ‘å¼‚å¸¸: {str(e)}")
            return None

    async def _fetch_report_detail(self, report_id: str) -> Optional[Dict[str, Any]]:
        """
        è·å–æŠ¥è¡¨è¯¦æƒ…ï¼ˆé€šè¿‡HTTPSæ¥å£ï¼‰

        Args:
            report_id: æŠ¥è¡¨ID

        Returns:
            æŠ¥è¡¨è¯¦æƒ…æ•°æ®
        """
        try:
            # è°ƒç”¨HTTPSæ¥å£è·å–æŠ¥è¡¨è¯¦æƒ…
            result = await self.https_client.get_table_detail(report_id)

            # æ£€æŸ¥å“åº”çŠ¶æ€
            if result.get("status_code") == 0:
                return result.get("data")

            error_msg = result.get("status_msg", "Unknown error")
            logger.error(f"è·å–æŠ¥è¡¨è¯¦æƒ…å¤±è´¥ (ID: {report_id}): {error_msg}")
            return None

        except Exception as e:
            logger.error(f"è·å–æŠ¥è¡¨è¯¦æƒ…å¼‚å¸¸ (ID: {report_id}): {str(e)}")
            return None

    def _build_report_yaml_data(self,
                               node: Dict[str, Any],
                               report_detail: Dict[str, Any],
                               path_parts: List[str],
                               export_time: str) -> Dict[str, Any]:
        """
        æ„å»ºæŠ¥è¡¨YAMLæ•°æ®

        Args:
            node: ç›®å½•æ ‘èŠ‚ç‚¹
            report_detail: æŠ¥è¡¨è¯¦æƒ…
            path_parts: è·¯å¾„ç»„æˆéƒ¨åˆ†
            export_time: å¯¼å‡ºæ—¶é—´

        Returns:
            YAMLæ•°æ®å­—å…¸
        """
        # åˆå§‹åŒ–ç©ºå­—å…¸
        yaml_data = {}

        # æŠ¥è¡¨è¯¦æƒ…ï¼šç›´æ¥å¹³é“ºï¼Œä¸ç”¨detailå°è£…
        if report_detail:
            yaml_data.update(report_detail)

        # éœ€è¦ç§»é™¤çš„å­—æ®µåˆ—è¡¨
        fields_to_remove = [
            "_path_info", "_metadata", "dashboard", "principal",
            "parent_unique_id", "db_name",
            "primary_column_list", "partition_column_list",
            "dir_id", "lifecycle", "create_time", "modifier",
            "creator", "update_time", "directory", "datasource_name",
            "enable_tag", "uuid", "puuid"
        ]

        # ç§»é™¤ä¸éœ€è¦çš„å­—æ®µ
        for field in fields_to_remove:
            yaml_data.pop(field, None)

        # é‡å‘½å column_list ä¸º columns
        if "column_list" in yaml_data:
            yaml_data["columns"] = yaml_data.pop("column_list")

        # ç‰¹æ®Šé€»è¾‘ï¼šå°† business_line ä¸­çš„ "dream" æ›¿æ¢ä¸º "dreamface"
        if "business_line" in yaml_data:
            business_lines = yaml_data["business_line"]
            if isinstance(business_lines, list):
                yaml_data["business_line"] = [
                    "dreamface" if bl == "dream" else bl
                    for bl in business_lines
                ]

        # æŒ‰æŒ‡å®šé¡ºåºé‡æ–°ç»„ç»‡å­—æ®µ
        ordered_data = {}

        # æ‰€æœ‰å­—æ®µæŒ‰é¡ºåº
        ordered_fields = ["id", "en_name", "cn_name", "standard_name", "alias",
                         "business_line", "table_type", "path"]

        for field in ordered_fields:
            if field in yaml_data:
                ordered_data[field] = yaml_data[field]

        # extra_info(å€’æ•°ç¬¬äºŒ) - è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²æ ¼å¼
        if "extra_info" in yaml_data:
            import json
            extra_info = yaml_data["extra_info"].copy()  # å¤åˆ¶ä¸€ä»½ï¼Œä¸ä¿®æ”¹åŸå§‹æ•°æ®
            show_type_desc = None  # ç”¨äºä¿å­˜showTypeçš„ä¸­æ–‡æè¿°

            if isinstance(extra_info, dict):
                # ç§»é™¤ env å­—æ®µ
                extra_info.pop("env", None)

                # showTypeæšä¸¾æ˜ å°„
                show_type_map = {
                    0: "è¡¨æ ¼", 1: "æŠ˜çº¿å›¾", 2: "æŒ‡æ ‡å¡ç‰‡", 3: "æŸ±çŠ¶å›¾",
                    4: "æ•£ç‚¹å›¾", 5: "æ¡çŠ¶å›¾", 6: "é¥¼å›¾", 7: "ç«ç‘°å›¾",
                    8: "å †å å›¾", 9: "ç€‘å¸ƒå›¾", 10: "æ¼æ–—å›¾", 11: "å¯Œæ–‡æœ¬",
                    12: "æ’è¡Œæ¦œ", 13: "é¢ç§¯å›¾", 14: "æ¡‘åŸºå›¾", 15: "å¾„å‘æ ‘çŠ¶å›¾",
                    16: "æ—­æ—¥å›¾"
                }

                # è·å–showTypeçš„ä¸­æ–‡æè¿°ï¼ˆç”¨äºæ³¨é‡Šï¼Œä¸æ·»åŠ åˆ°æ•°æ®ä¸­ï¼‰
                if "showType" in extra_info:
                    show_type_value = extra_info["showType"]
                    show_type_desc = show_type_map.get(show_type_value, f"æœªçŸ¥ç±»å‹({show_type_value})")

                # ä½¿ç”¨json.dumpsæ ¼å¼åŒ–ä¸ºå¤šè¡ŒJSONå­—ç¬¦ä¸²
                json_str = json.dumps(extra_info, indent=2, ensure_ascii=False)
                # ä½¿ç”¨ç‰¹æ®Šæ ‡è®°ï¼Œåç»­å¤„ç†æ—¶è½¬æ¢ä¸ºliteral block scalar
                ordered_data["extra_info"] = f"__LITERAL_BLOCK_START__\n{json_str}\n__LITERAL_BLOCK_END__"
                # ä¿å­˜showTypeæè¿°ç”¨äºæ³¨é‡Š
                ordered_data["_show_type_desc"] = show_type_desc
            else:
                ordered_data["extra_info"] = extra_info

        # columns - è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²æ ¼å¼
        if "columns" in yaml_data:
            import json
            columns = yaml_data["columns"]
            if isinstance(columns, list):
                # æ ¼å¼åŒ–æ¯ä¸ªå­—æ®µå¯¹è±¡
                formatted_columns = []
                for col in columns:
                    # åªä¿ç•™éœ€è¦çš„å­—æ®µï¼ˆç§»é™¤ order_noï¼‰
                    simplified_col = {
                        "en_name": col.get("en_name"),
                        "cn_name": col.get("cn_name"),
                        "type": col.get("type"),
                        "sample_data": col.get("sample_data")
                    }
                    formatted_columns.append(simplified_col)

                # ä½¿ç”¨json.dumpsæ ¼å¼åŒ–ä¸ºå¤šè¡ŒJSONå­—ç¬¦ä¸²
                json_str = json.dumps(formatted_columns, indent=2, ensure_ascii=False)
                # ä½¿ç”¨ç‰¹æ®Šæ ‡è®°ï¼Œåç»­å¤„ç†æ—¶è½¬æ¢ä¸ºliteral block scalar
                ordered_data["columns"] = f"__LITERAL_BLOCK_START__\n{json_str}\n__LITERAL_BLOCK_END__"
            else:
                ordered_data["columns"] = columns

        return ordered_data

    def _save_report_yaml_file(self, yaml_data: Dict[str, Any], file_path: Path) -> bool:
        """
        ä¿å­˜æŠ¥è¡¨YAMLæ–‡ä»¶ï¼ˆè‡ªå®šä¹‰æ ¼å¼ï¼‰

        Args:
            yaml_data: YAMLæ•°æ®
            file_path: æ–‡ä»¶è·¯å¾„

        Returns:
            bool: æ˜¯å¦ä¿å­˜æˆåŠŸ
        """
        try:
            import yaml
            from io import StringIO
            from ..utils.file_manager import QuotedStringDumper

            # åˆ›å»ºç›®å½•ç»“æ„
            file_path.parent.mkdir(parents=True, exist_ok=True)

            # ä¿å­˜showTypeæè¿°å¹¶ä»æ•°æ®ä¸­ç§»é™¤
            show_type_desc = yaml_data.pop("_show_type_desc", None)

            # ç”ŸæˆYAMLå†…å®¹
            stream = StringIO()
            yaml.dump(yaml_data, stream,
                     Dumper=QuotedStringDumper,
                     default_flow_style=False,
                     allow_unicode=True,
                     indent=2,
                     sort_keys=False)
            yaml_content = stream.getvalue()

            # å¤„ç†ç‰¹æ®Šæ ‡è®°ï¼Œç§»é™¤æ ‡è®°è¡Œä½†ä¿ç•™å†…å®¹
            import re

            # ç§»é™¤ __LITERAL_BLOCK_START__ å’Œ __LITERAL_BLOCK_END__ æ ‡è®°è¡Œ
            lines = yaml_content.split('\n')
            cleaned_lines = []
            for line in lines:
                # è·³è¿‡åŒ…å«æ ‡è®°çš„è¡Œ
                if '__LITERAL_BLOCK_START__' in line or '__LITERAL_BLOCK_END__' in line:
                    continue
                cleaned_lines.append(line)

            yaml_content = '\n'.join(cleaned_lines)

            # åŠ¨æ€æ„å»ºextra_infoæ³¨é‡Šï¼ˆä½¿ç”¨ä¹‹å‰ä¿å­˜çš„show_type_descï¼‰
            if show_type_desc:
                extra_info_comment = f'  # æ‰©å±•ä¿¡æ¯(uv: è¿‘3ä¸ªæœˆæŠ¥è¡¨ä½¿ç”¨uv, pv: è¿‘3ä¸ªæœˆæŠ¥è¡¨ä½¿ç”¨pv, showType: æŠ¥è¡¨å›¾è¡¨ç±»å‹-{show_type_desc})'
            else:
                extra_info_comment = '  # æ‰©å±•ä¿¡æ¯(uv: è¿‘3ä¸ªæœˆæŠ¥è¡¨ä½¿ç”¨uv, pv: è¿‘3ä¸ªæœˆæŠ¥è¡¨ä½¿ç”¨pv, showType: æŠ¥è¡¨å›¾è¡¨ç±»å‹)'

            # æ·»åŠ å­—æ®µæ³¨é‡Š
            field_comments = {
                '"en_name":': '  # æŠ¥è¡¨è‹±æ–‡åç§°',
                '"cn_name":': '  # æŠ¥è¡¨ä¸­æ–‡åç§°',
                '"standard_name":': '  # æŠ¥è¡¨æ²»ç†åçš„æ ‡å‡†åç§°',
                '"alias":': '  # æŠ¥è¡¨åˆ«åï¼Œå¤šä¸ªç”¨è‹±æ–‡é€—å·åˆ†éš”',
                '"business_line":': '  # å½’å±ä¸šåŠ¡çº¿',
                '"path":': '  # æŠ¥è¡¨è·¯å¾„',
                '"extra_info":': extra_info_comment,
                '"columns":': '  # æŠ¥è¡¨å­—æ®µåˆ—è¡¨'
            }

            # ä¸ºæ¯ä¸ªå­—æ®µæ·»åŠ æ³¨é‡Šï¼ˆåªä¸ºé¡¶å±‚å­—æ®µæ·»åŠ ï¼Œä¸ä¸ºåµŒå¥—å­—æ®µæ·»åŠ ï¼‰
            lines = yaml_content.split('\n')
            commented_lines = []
            in_multiline_block = False  # æ ‡è®°æ˜¯å¦åœ¨å¤šè¡Œå—å†…ï¼ˆextra_info æˆ– columnsï¼‰

            for line in lines:
                # æ£€æŸ¥æ˜¯å¦è¿›å…¥å¤šè¡Œå—
                if '"extra_info": |' in line or '"columns": |' in line:
                    in_multiline_block = True
                    # ä¸ºè¯¥é¡¶å±‚å­—æ®µæ·»åŠ æ³¨é‡Š
                    for field, comment in field_comments.items():
                        if line.strip().startswith(field):
                            commented_lines.append(line.rstrip() + comment)
                            break
                    else:
                        commented_lines.append(line)
                    continue

                # å¦‚æœåœ¨å¤šè¡Œå—å†…
                if in_multiline_block:
                    # æ£€æŸ¥æ˜¯å¦ç¦»å¼€å¤šè¡Œå—ï¼ˆä¸‹ä¸€ä¸ªé¡¶å±‚å­—æ®µå‡ºç°ï¼Œæ²¡æœ‰ç¼©è¿›ï¼‰
                    if line and not line[0].isspace() and line.strip().startswith('"'):
                        in_multiline_block = False
                        # å¤„ç†è¿™ä¸ªæ–°çš„é¡¶å±‚å­—æ®µ
                        comment_added = False
                        for field, comment in field_comments.items():
                            if line.strip().startswith(field):
                                commented_lines.append(line + comment)
                                comment_added = True
                                break
                        if not comment_added:
                            commented_lines.append(line)
                    else:
                        # ä»åœ¨å¤šè¡Œå—å†…ï¼Œä¸æ·»åŠ æ³¨é‡Š
                        commented_lines.append(line)
                    continue

                # ä¸ºé¡¶å±‚å­—æ®µæ·»åŠ æ³¨é‡Š
                comment_added = False
                for field, comment in field_comments.items():
                    if line.strip().startswith(field):
                        commented_lines.append(line + comment)
                        comment_added = True
                        break

                if not comment_added:
                    commented_lines.append(line)

            yaml_content = '\n'.join(commented_lines)

            # å†™å…¥æ–‡ä»¶
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(yaml_content)

            logger.debug(f"YAMLæ–‡ä»¶å·²ä¿å­˜: {file_path}")
            return True

        except Exception as e:
            logger.error(f"ä¿å­˜YAMLæ–‡ä»¶å¤±è´¥: {file_path}, é”™è¯¯: {str(e)}")
            return False
